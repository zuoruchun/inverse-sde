{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小: 152.59 MB\n",
      "数据形状: (20000000, 2)\n",
      "数据类型: float32\n",
      "数据总数: 40000000\n",
      "维度数量: 2\n",
      "\n",
      "数据统计信息:\n",
      "最小值: [-45.830338 -98.97168 ]\n",
      "最大值: [59.15394 64.02583]\n",
      "均值: [0.00324247 0.00250749]\n",
      "标准差: [1.1309026 1.6522223]\n",
      "============================\n",
      "\n",
      "数据统计信息:\n",
      "最小值: [-3.9999886 -5.9999766]\n",
      "最大值: [3.9999998 5.999994 ]\n",
      "均值: [0.00100681 0.00066653]\n",
      "标准差: [0.98415375 1.4839878 ]\n",
      "Number of points in Omega: 19734518 (98.67%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# 禁用matplotlib字体管理器的警告\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "# 设置中文字体\n",
    "ukai_font = fm.FontProperties(fname='/usr/share/fonts/truetype/arphic/ukai.ttc')\n",
    "\n",
    "# 显示文件信息\n",
    "file_path = 'sde_data_cuda.npy'\n",
    "file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "print(f\"文件大小: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# 读取数据\n",
    "data = np.load(file_path)\n",
    "N = data.shape[0]\n",
    "\n",
    "# 显示数据详情\n",
    "print(f\"数据形状: {data.shape}\")\n",
    "print(f\"数据类型: {data.dtype}\")\n",
    "print(f\"数据总数: {data.size}\")\n",
    "print(f\"维度数量: {data.ndim}\")\n",
    "\n",
    "# 显示数据统计信息\n",
    "print(\"\\n数据统计信息:\")\n",
    "print(f\"最小值: {data.min(axis=0)}\")\n",
    "print(f\"最大值: {data.max(axis=0)}\")\n",
    "print(f\"均值: {data.mean(axis=0)}\")\n",
    "print(f\"标准差: {data.std(axis=0)}\")\n",
    "\n",
    "\n",
    "\n",
    "mask_x1 = (data[:, 0] >= -4) & (data[:, 0] <= 4) \n",
    "mask_x2 = (data[:, 1] >= -6) & (data[:, 1] <= 6)\n",
    "mask = mask_x1 & mask_x2\n",
    "data = data[mask]\n",
    "print('============================')\n",
    "# 显示数据统计信息\n",
    "print(\"\\n数据统计信息:\")\n",
    "print(f\"最小值: {data.min(axis=0)}\")\n",
    "print(f\"最大值: {data.max(axis=0)}\")\n",
    "print(f\"均值: {data.mean(axis=0)}\")\n",
    "print(f\"标准差: {data.std(axis=0)}\")\n",
    "\n",
    "print(f\"Number of points in Omega: {len(data)} ({len(data)/N*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data from sde_data_cuda.npy...\n",
      "Loaded 20000000 data points\n",
      "Initializing models...\n",
      "Training drift network from time series data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1016/20000 [00:11<03:40, 86.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/20000, Loss: 1675.458008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2011/20000 [00:22<03:19, 89.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000/20000, Loss: 1748.650391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3016/20000 [00:34<03:11, 88.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000/20000, Loss: 1714.260742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4010/20000 [00:45<02:51, 93.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000/20000, Loss: 1671.008301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5017/20000 [00:56<02:47, 89.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000/20000, Loss: 1666.587402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6009/20000 [01:09<03:01, 77.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000/20000, Loss: 1745.371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7009/20000 [01:22<02:47, 77.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000/20000, Loss: 1608.739380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8011/20000 [01:34<02:15, 88.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8000/20000, Loss: 1700.186768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9015/20000 [01:46<02:03, 89.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9000/20000, Loss: 1721.941528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10010/20000 [01:57<01:53, 87.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000/20000, Loss: 1657.847412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11013/20000 [02:08<01:41, 88.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11000/20000, Loss: 1745.169922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12011/20000 [02:19<01:29, 88.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12000/20000, Loss: 1666.066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13015/20000 [02:31<01:26, 80.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13000/20000, Loss: 1714.924561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14015/20000 [02:43<01:16, 78.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14000/20000, Loss: 1699.047729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15016/20000 [02:54<00:53, 92.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000/20000, Loss: 1671.987915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16011/20000 [03:05<00:44, 90.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16000/20000, Loss: 1672.401611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17017/20000 [03:17<00:30, 98.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17000/20000, Loss: 1639.760010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18015/20000 [03:28<00:22, 89.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18000/20000, Loss: 1738.780518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19011/20000 [03:39<00:11, 88.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19000/20000, Loss: 1593.578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:50<00:00, 86.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000/20000, Loss: 1649.545776\n",
      "Drift relative L2 errors - a1: 9.7590e-01, a2: 1.0064e+00, total: 9.9224e-01\n",
      "Total drift network error: 0.992241\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 设置随机种子以确保可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. 数据生成函数 - 使用EM方案生成时间序列\n",
    "def phi(x1, x2):\n",
    "    \"\"\"计算phi(x1, x2) = 1 + (2/15)(4x1^2 - x1*x2 + x2^2)\"\"\"\n",
    "    return 1 + (2/15) * (4*x1**2 - x1*x2 + x2**2)\n",
    "\n",
    "def drift(x):\n",
    "    \"\"\"漂移项 a(x)\"\"\"\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    a1 = -1.5*x1 + x2\n",
    "    a2 = 0.25*x1 - 1.5*x2\n",
    "    return torch.stack([a1, a2], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. 定义神经网络模型\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish激活函数: x * tanh(softplus(x))\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "        # return x * torch.tanh(torch.log(1 + torch.exp(x)))\n",
    "\n",
    "class ReLU3(nn.Module):\n",
    "    \"\"\"ReLU3激活函数: min(max(0,x), 3)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(x), max=3.0)\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"残差网络块\"\"\"\n",
    "    def __init__(self, dim, width, activation='mish'):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.lin1 = nn.Linear(dim, width)\n",
    "        self.lin2 = nn.Linear(width, dim)\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "\n",
    "        # Kaiming 初始化\n",
    "        nn.init.kaiming_normal_(self.lin1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.lin2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin1.bias)\n",
    "        nn.init.zeros_(self.lin2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"残差网络\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=6, activation='mish'):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetBlock(hidden_dim, hidden_dim, activation) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "class DriftNet(nn.Module):\n",
    "    \"\"\"估计漂移项的网络 - 使用ReLU激活函数\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=50, output_dim=2, num_blocks=6):\n",
    "        super(DriftNet, self).__init__()\n",
    "        self.net = ResNet(input_dim, hidden_dim, output_dim, num_blocks, activation='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# 3. 训练函数\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_drift_net_from_data(a_nn, train_data, dt=0.05, num_iterations=20000, batch_size=10000, lr=1e-4, omega_bounds=([-4, 4], [-6, 6])):\n",
    "    \"\"\"训练漂移项网络 - 使用时间序列数据中的实际位移\"\"\"\n",
    "    print(\"Training drift network from time series data...\")\n",
    "    a_nn.to(device)\n",
    "    optimizer = optim.Adam(a_nn.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iterations)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # 准备数据：计算位移 y = (x_{n+1} - x_n)/dt\n",
    "    N = len(train_data)\n",
    "    y_data = np.zeros((N-1, train_data.shape[1]))  # 初始化位移数组\n",
    "    for n in range(N-1):\n",
    "        y_data[n] = (train_data[n+1] - train_data[n]) / dt\n",
    "    \n",
    "    mask_x1 = (train_data[:, 0] >= -4) & (train_data[:, 0] <= 4) \n",
    "    mask_x2 = (train_data[:, 1] >= -6) & (train_data[:, 1] <= 6)\n",
    "    mask = mask_x1 & mask_x2\n",
    "    train_data = train_data[mask]\n",
    "\n",
    "    # 对应的x点取前N-1个点\n",
    "    x_data = train_data[:-1]\n",
    "    \n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        # 随机抽取批次\n",
    "        idx = np.random.choice(len(x_data), batch_size)\n",
    "        x_batch = torch.tensor(x_data[idx], dtype=torch.float32, device=device)\n",
    "        y_batch = torch.tensor(y_data[idx], dtype=torch.float64, device=device)\n",
    "        \n",
    "        # 前向传播\n",
    "        a_pred = a_nn(x_batch)\n",
    "        \n",
    "        # 计算损失：直接计算所有维度的 MSE\n",
    "        loss = torch.mean((a_pred - y_batch) ** 2)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(a_nn.parameters(), max_norm=10.0)      # 梯度裁剪\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_grid_points(nx=300, ny=300):\n",
    "    \"\"\"生成评估网格点\"\"\"\n",
    "    x = np.linspace(-4, 4, nx)\n",
    "    y = np.linspace(-6, 6, ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 将网格点展平为坐标点列表\n",
    "    points = np.column_stack([X.ravel(), Y.ravel()])\n",
    "    return points, X, Y\n",
    "\n",
    "def compute_relative_L2_error(func_true, func_pred, grid_points):\n",
    "    \"\"\"计算相对L2误差\"\"\"\n",
    "    true_vals = func_true(grid_points)\n",
    "    pred_vals = func_pred(grid_points)\n",
    "    \n",
    "    error = np.sqrt(np.mean((true_vals - pred_vals)**2))\n",
    "    norm = np.sqrt(np.mean(true_vals**2))\n",
    "    \n",
    "    return error / norm\n",
    "\n",
    "# 4. 评估函数\n",
    "def evaluate_drift_net(a_nn, grid_points, X, Y):\n",
    "    \"\"\"评估漂移项网络\"\"\"\n",
    "    # 转换为张量\n",
    "    x_tensor = torch.tensor(grid_points, dtype=torch.float64, device=device)\n",
    "    \n",
    "    # 计算真实值和预测值\n",
    "    with torch.no_grad():\n",
    "        a_true = drift(x_tensor).cpu().numpy()\n",
    "        a_pred = a_nn(x_tensor).cpu().numpy()\n",
    "    \n",
    "    # 计算相对L2误差\n",
    "    error_a1 = np.sqrt(np.mean((a_true[:, 0] - a_pred[:, 0])**2)) / np.sqrt(np.mean(a_true[:, 0]**2))\n",
    "    error_a2 = np.sqrt(np.mean((a_true[:, 1] - a_pred[:, 1])**2)) / np.sqrt(np.mean(a_true[:, 1]**2))\n",
    "    total_error = np.sqrt(np.mean(np.sum((a_true - a_pred)**2, axis=1))) / np.sqrt(np.mean(np.sum(a_true**2, axis=1)))\n",
    "    \n",
    "    print(f\"Drift relative L2 errors - a1: {error_a1:.4e}, a2: {error_a2:.4e}, total: {total_error:.4e}\")\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 第一个分量\n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a1')\n",
    "    plt.title('True Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a1')\n",
    "    plt.title('Predicted Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 0] - a_pred[:, 0]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a1')\n",
    "    plt.title(f'Error in Drift a1 (L2 rel. error: {error_a1:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a1_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 第二个分量\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a2')\n",
    "    plt.title('True Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a2')\n",
    "    plt.title('Predicted Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 1] - a_pred[:, 1]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a2')\n",
    "    plt.title(f'Error in Drift a2 (L2 rel. error: {error_a2:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a2_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "\n",
    "# 5. 主函数\n",
    "def main():\n",
    "    # 创建输出目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # 参数设置\n",
    "    dt = 0.05  # 时间步长\n",
    "    N = 2*10**7  # 数据点数 (实际实现中可能需要减少点数以加快运算)\n",
    "    hidden_dim = 50  # 隐藏层维度\n",
    "    num_blocks = 6   # 残差网络块数量\n",
    "    \n",
    "    # 训练参数\n",
    "    num_iterations = 20000  # 训练迭代次数\n",
    "    batch_size = 10000      # 批处理大小\n",
    "    learning_rate = 1e-4   # 学习率\n",
    "    \n",
    "    # 1. 数据准备\n",
    "    data_file = \"sde_data_cuda.npy\"\n",
    "\n",
    "    print(f\"Loading data from {data_file}...\")\n",
    "    data = np.load(data_file)\n",
    "    print(f\"Loaded {len(data)} data points\")\n",
    "\n",
    "    \n",
    "    # 2. 准备评估网格\n",
    "    grid_points, X, Y = generate_grid_points(nx=100, ny=100)\n",
    "    \n",
    "    # 3. 初始化模型\n",
    "    print(\"Initializing models...\")\n",
    "    drift_net = DriftNet(input_dim=2, hidden_dim=hidden_dim, output_dim=2, num_blocks=num_blocks).to(device)\n",
    "    # diffusion_net = DiffusionNet(input_dim=2, hidden_dim=hidden_dim, output_dim=3, num_blocks=num_blocks).to(device)\n",
    "    # density_net = DensityNet(input_dim=2, hidden_dim=hidden_dim, output_dim=1, num_blocks=num_blocks).to(device)\n",
    "    \n",
    "    # 4. 训练和评估模型\n",
    "    # 4.1 训练漂移项网络\n",
    "    drift_losses = train_drift_net_from_data(\n",
    "        drift_net, \n",
    "        data, \n",
    "        dt=dt, \n",
    "        num_iterations=num_iterations, \n",
    "        batch_size=batch_size, \n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(drift_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Drift Network Training Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/drift_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 评估漂移项网络\n",
    "    drift_error = evaluate_drift_net(drift_net, grid_points, X, Y)\n",
    "    print(f\"Total drift network error: {drift_error:.6f}\")\n",
    "\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.模型与设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 设置随机种子以确保可复现性\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def drift(x):\n",
    "    \"\"\"漂移项 a(x)\"\"\"\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    a1 = -1.5*x1 + x2\n",
    "    a2 = 0.25*x1 - 1.5*x2\n",
    "    return torch.stack([a1, a2], dim=1)\n",
    "\n",
    "# 2. 定义神经网络模型\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish激活函数: x * tanh(softplus(x))\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "        # return x * torch.tanh(torch.log(1 + torch.exp(x)))\n",
    "\n",
    "class ReLU3(nn.Module):\n",
    "    \"\"\"ReLU3激活函数: min(max(0,x), 3)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(x), max=3.0)\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"残差网络块\"\"\"\n",
    "    def __init__(self, dim, width, activation='mish'):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.lin1 = nn.Linear(dim, width)\n",
    "        self.lin2 = nn.Linear(width, dim)\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "\n",
    "        # Kaiming 初始化\n",
    "        nn.init.kaiming_normal_(self.lin1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.lin2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.lin1.bias)\n",
    "        nn.init.zeros_(self.lin2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"残差网络\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=6, activation='mish'):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetBlock(hidden_dim, hidden_dim, activation) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input_layer(x))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "class DriftNet(nn.Module):\n",
    "    \"\"\"估计漂移项的网络 - 使用ReLU激活函数\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=50, output_dim=2, num_blocks=6):\n",
    "        super(DriftNet, self).__init__()\n",
    "        self.net = ResNet(input_dim, hidden_dim, output_dim, num_blocks, activation='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 3. 训练函数\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_grid_points(nx=300, ny=300, for_eval=False):\n",
    "    \"\"\"生成评估网格点\n",
    "    \n",
    "    参数:\n",
    "        nx, ny: 网格点数量\n",
    "        for_eval: 如果为True，则生成10,000个点（用于计算相对L2误差）\n",
    "    \"\"\"\n",
    "    if for_eval:\n",
    "        # 为了计算相对L2误差，生成10,000个高斯求积点\n",
    "        # 这里简单地生成均匀网格点，实际上应该使用高斯求积点\n",
    "        num_points = 10000\n",
    "        x_points = np.random.uniform(-4, 4, num_points)\n",
    "        y_points = np.random.uniform(-6, 6, num_points)\n",
    "        points = np.column_stack([x_points, y_points])\n",
    "        return points, None, None\n",
    "    else:\n",
    "        # 为可视化生成网格点\n",
    "        x = np.linspace(-4, 4, nx)\n",
    "        y = np.linspace(-6, 6, ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # 将网格点展平为坐标点列表\n",
    "        points = np.column_stack([X.ravel(), Y.ravel()])\n",
    "        return points, X, Y\n",
    "\n",
    "def compute_relative_L2_error(func_true, func_pred, grid_points):\n",
    "    \"\"\"计算相对L2误差，使用10,000个高斯求积点\"\"\"\n",
    "    true_vals = func_true(grid_points)\n",
    "    pred_vals = func_pred(grid_points)\n",
    "    \n",
    "    error = np.sqrt(np.mean((true_vals - pred_vals)**2))\n",
    "    norm = np.sqrt(np.mean(true_vals**2))\n",
    "    \n",
    "    return error / norm\n",
    "\n",
    "# 4. 评估函数\n",
    "def evaluate_drift_net(a_nn, grid_points, X, Y):\n",
    "    \"\"\"评估漂移项网络\"\"\"\n",
    "    # 生成用于计算相对L2误差的10,000个高斯求积点\n",
    "    eval_points, _, _ = generate_grid_points(for_eval=True)\n",
    "    \n",
    "    # 转换为张量\n",
    "    x_tensor = torch.tensor(grid_points, dtype=torch.float64, device=device)\n",
    "    eval_tensor = torch.tensor(eval_points, dtype=torch.float64, device=device)\n",
    "    \n",
    "    # 计算真实值和预测值（用于可视化）\n",
    "    with torch.no_grad():\n",
    "        a_true = drift(x_tensor).cpu().numpy()\n",
    "        a_pred = a_nn(x_tensor).cpu().numpy()\n",
    "        \n",
    "        # 计算用于误差的真实值和预测值\n",
    "        a_true_eval = drift(eval_tensor).cpu().numpy()\n",
    "        a_pred_eval = a_nn(eval_tensor).cpu().numpy()\n",
    "    \n",
    "    # 计算相对L2误差\n",
    "    error_a1 = np.sqrt(np.mean((a_true_eval[:, 0] - a_pred_eval[:, 0])**2)) / np.sqrt(np.mean(a_true_eval[:, 0]**2))\n",
    "    error_a2 = np.sqrt(np.mean((a_true_eval[:, 1] - a_pred_eval[:, 1])**2)) / np.sqrt(np.mean(a_true_eval[:, 1]**2))\n",
    "    total_error = np.sqrt(np.mean(np.sum((a_true_eval - a_pred_eval)**2, axis=1))) / np.sqrt(np.mean(np.sum(a_true_eval**2, axis=1)))\n",
    "    \n",
    "    print(f\"Drift relative L2 errors - a1: {error_a1:.4e}, a2: {error_a2:.4e}, total: {total_error:.4e}\")\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 第一个分量\n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a1')\n",
    "    plt.title('True Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a1')\n",
    "    plt.title('Predicted Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 0] - a_pred[:, 0]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a1')\n",
    "    plt.title(f'Error in Drift a1 (L2 rel. error: {error_a1:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a1_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 第二个分量\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a2')\n",
    "    plt.title('True Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a2')\n",
    "    plt.title('Predicted Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 1] - a_pred[:, 1]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a2')\n",
    "    plt.title(f'Error in Drift a2 (L2 rel. error: {error_a2:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a2_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return total_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_drift_net_from_data(a_nn, train_data, dt=0.05, num_iterations=20000, batch_size=10000, lr=1e-4, omega_bounds=([-4, 4], [-6, 6])):\n",
    "    \"\"\"训练漂移项网络 - 使用时间序列数据中的实际位移\"\"\"\n",
    "    print(\"训练漂移项网络...\")\n",
    "    a_nn.to(device)\n",
    "    optimizer = optim.Adam(a_nn.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iterations)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # 首先筛选 Omega 内的点\n",
    "    mask_x1 = (train_data[:, 0] >= omega_bounds[0][0]) & (train_data[:, 0] <= omega_bounds[0][1]) \n",
    "    mask_x2 = (train_data[:, 1] >= omega_bounds[1][0]) & (train_data[:, 1] <= omega_bounds[1][1])\n",
    "    mask = mask_x1 & mask_x2\n",
    "    filtered_data = train_data[mask]\n",
    "    \n",
    "    # 显示数据统计信息\n",
    "    print(\"\\n筛选后的数据统计信息:\")\n",
    "    print(f\"数据点数量: {filtered_data.shape[0]} ({filtered_data.shape[0]/len(train_data)*100:.2f}%)\")\n",
    "    print(f\"最小值: {filtered_data.min(axis=0)}\")\n",
    "    print(f\"最大值: {filtered_data.max(axis=0)}\")\n",
    "    print(f\"均值: {filtered_data.mean(axis=0)}\")\n",
    "    print(f\"标准差: {filtered_data.std(axis=0)}\")\n",
    "\n",
    "    # 准备输入和目标数据\n",
    "    N = len(filtered_data)\n",
    "    x_data = filtered_data[:-1]  # 取前N-1个点作为输入\n",
    "    \n",
    "    # 计算位移 y = (x_{n+1} - x_n)/dt\n",
    "    y_data = np.zeros((N-1, filtered_data.shape[1]))\n",
    "    for n in range(N-1):\n",
    "        y_data[n] = (filtered_data[n+1] - filtered_data[n]) / dt\n",
    "    \n",
    "    print(f\"训练数据准备完成: x_data: {x_data.shape}, y_data: {y_data.shape}\")\n",
    "    \n",
    "    # 检查 y_data 的数值范围\n",
    "    print(f\"位移数据统计: min={y_data.min(axis=0)}, max={y_data.max(axis=0)}, mean={y_data.mean(axis=0)}, std={y_data.std(axis=0)}\")\n",
    "    \n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        # 随机抽取批次\n",
    "        idx = np.random.choice(len(x_data), batch_size)\n",
    "        x_batch = torch.tensor(x_data[idx], dtype=torch.float64, device=device)\n",
    "        y_batch = torch.tensor(y_data[idx], dtype=torch.float64, device=device)\n",
    "        \n",
    "        # 前向传播\n",
    "        a_pred = a_nn(x_batch)\n",
    "        \n",
    "        # 计算损失：直接计算所有维度的 MSE\n",
    "        loss = torch.mean((a_pred - y_batch) ** 2)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"迭代 {i+1}/{num_iterations}, 损失: {loss.item():.6f}\")\n",
    "    \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "复现论文中的Student's t-distribution实验\n",
      "==================================================\n",
      "\n",
      "加载已有数据: sde_data_cuda.npy...\n",
      "加载了 20000000 个数据点\n",
      "生成评估网格...\n",
      "初始化神经网络模型...\n",
      "\n",
      "------------------------------\n",
      "第一步: 训练漂移项网络\n",
      "------------------------------\n",
      "训练漂移项网络...\n",
      "\n",
      "筛选后的数据统计信息:\n",
      "数据点数量: 19734518 (98.67%)\n",
      "最小值: [-3.9999886 -5.9999766]\n",
      "最大值: [3.9999998 5.999994 ]\n",
      "均值: [0.00100681 0.00066653]\n",
      "标准差: [0.98415375 1.4839878 ]\n",
      "训练数据准备完成: x_data: (19734517, 2), y_data: (19734517, 2)\n",
      "位移数据统计: min=[-159.50411987 -237.2303009 ], max=[158.23001099 235.92623901], mean=[1.94006316e-08 5.45391047e-07], std=[28.46923542 42.96396005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1011/20000 [00:11<04:03, 77.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 1000/20000, 损失: 663.164307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2010/20000 [00:23<03:22, 88.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 2000/20000, 损失: 661.026184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3009/20000 [00:34<03:11, 88.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 3000/20000, 损失: 639.852112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4016/20000 [00:45<02:58, 89.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 4000/20000, 损失: 652.023071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5015/20000 [00:57<02:46, 90.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 5000/20000, 损失: 646.022034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6016/20000 [01:08<02:39, 87.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 6000/20000, 损失: 680.613953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7011/20000 [01:19<02:25, 89.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 7000/20000, 损失: 665.172058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8017/20000 [01:30<02:11, 91.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 8000/20000, 损失: 651.412537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9014/20000 [01:42<02:09, 84.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 9000/20000, 损失: 664.242676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10014/20000 [01:53<01:52, 88.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 10000/20000, 损失: 658.221008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11016/20000 [02:04<01:40, 89.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 11000/20000, 损失: 678.550842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12014/20000 [02:15<01:29, 89.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 12000/20000, 损失: 672.192566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13017/20000 [02:26<01:16, 91.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 13000/20000, 损失: 653.649475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14013/20000 [02:37<01:07, 88.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 14000/20000, 损失: 661.315002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15014/20000 [02:49<00:56, 87.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 15000/20000, 损失: 673.820557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16015/20000 [03:00<00:44, 88.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 16000/20000, 损失: 671.298035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17013/20000 [03:11<00:33, 89.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 17000/20000, 损失: 661.399597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18014/20000 [03:23<00:21, 91.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 18000/20000, 损失: 653.968506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19010/20000 [03:34<00:11, 88.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 19000/20000, 损失: 654.255554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:45<00:00, 88.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代 20000/20000, 损失: 669.372803\n",
      "\n",
      "评估漂移项网络...\n",
      "Drift relative L2 errors - a1: 8.6429e+00, a2: 1.2258e+01, total: 1.0724e+01\n",
      "漂移项网络相对L2误差: 1.0724e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 主函数\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"复现论文中的Student's t-distribution实验\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # 参数设置 - 按照论文要求\n",
    "    dt = 0.05  # 时间步长\n",
    "    N = 2*10**7  # 数据点数\n",
    "    hidden_dim = 50  # 隐藏层维度\n",
    "    num_blocks = 6   # 残差网络块数量（6层隐藏层）\n",
    "    \n",
    "    # 训练参数 - 根据论文设置\n",
    "    num_iterations = 20000  # 训练迭代次数\n",
    "    drift_batch_size = 10000      # 漂移项批处理大小\n",
    "\n",
    "    \n",
    "    # 学习率 - 根据论文设置\n",
    "    drift_lr = 1e-4        # 漂移项学习率\n",
    "\n",
    "    \n",
    "    # 1. 数据准备\n",
    "    data_file = \"sde_data_cuda.npy\"\n",
    "    if os.path.exists(data_file):\n",
    "        print(f\"加载已有数据: {data_file}...\")\n",
    "        data = np.load(data_file)\n",
    "        # 如果需要，可以随机采样减少数据量\n",
    "        if len(data) > N:\n",
    "            indices = np.random.choice(len(data), N, replace=False)\n",
    "            data = data[indices]\n",
    "        print(f\"加载了 {len(data)} 个数据点\")\n",
    "    else:\n",
    "        print(\"生成新数据...\")\n",
    "        data = generate_data(dt=dt, N=N)\n",
    "        np.save(data_file, data)\n",
    "    \n",
    "    # 2. 准备评估网格\n",
    "    print(\"生成评估网格...\")\n",
    "    # 可视化用的网格 (100x100)\n",
    "    vis_grid_points, X, Y = generate_grid_points(nx=100, ny=100)\n",
    "    # 评估误差用的网格 (10,000高斯求积点)\n",
    "    eval_grid_points, _, _ = generate_grid_points(for_eval=True)\n",
    "    \n",
    "    # 3. 初始化模型\n",
    "    print(\"初始化神经网络模型...\")\n",
    "    drift_net = DriftNet(input_dim=2, hidden_dim=hidden_dim, output_dim=2, num_blocks=num_blocks).to(device)\n",
    "    \n",
    "    # 4. 训练和评估模型\n",
    "    # 4.1 训练漂移项网络\n",
    "    print(\"\\n\" + \"-\"*30)\n",
    "    print(\"第一步: 训练漂移项网络\")\n",
    "    print(\"-\"*30)\n",
    "    drift_losses = train_drift_net_from_data(\n",
    "        drift_net, \n",
    "        data, \n",
    "        dt=dt, \n",
    "        num_iterations=num_iterations, \n",
    "        batch_size=drift_batch_size, \n",
    "        lr=drift_lr\n",
    "    )\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(drift_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Drift Network Training Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/drift_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 评估漂移项网络\n",
    "    print(\"\\n评估漂移项网络...\")\n",
    "    drift_error = evaluate_drift_net(drift_net, vis_grid_points, X, Y)\n",
    "    print(f\"漂移项网络相对L2误差: {drift_error:.4e}\")\n",
    "\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不使用归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "复现论文中的Student's t-distribution实验\n",
      "==================================================\n",
      "\n",
      "生成数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 978.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选和预处理数据...\n",
      "筛选后的数据点数量: 19613606\n",
      "使用CUDA计算位移...\n",
      "筛选后数据点数量: 19613605 (98.07%)\n",
      "筛选后数据统计: min=[-3.99999738 -5.99999714], max=[3.99996138 5.99999332], mean=[0.00052919 0.00213004], std=[0.9358114  1.77695971]\n",
      "位移数据统计: min=[-158.02601337 -239.70229149], max=[159.75433826 236.28251076], mean=[-9.36094999e-08  1.93198790e-06], std=[26.4632473  50.25040997]\n",
      "生成网格点用于可视化和评估...\n",
      "\n",
      "------------------------------\n",
      "第一步: 训练漂移项网络\n",
      "------------------------------\n",
      "训练漂移项网络...\n",
      "将训练数据预加载到GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1009/20000 [00:12<03:35, 88.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/20000, Loss: 812.459388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2014/20000 [00:23<03:17, 91.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2000/20000, Loss: 808.403334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3010/20000 [00:34<03:08, 90.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3000/20000, Loss: 804.450533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4019/20000 [00:44<02:48, 94.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4000/20000, Loss: 811.838195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5009/20000 [00:55<02:44, 91.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000/20000, Loss: 801.061417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6018/20000 [01:06<02:25, 95.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6000/20000, Loss: 805.573430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7018/20000 [01:16<02:17, 94.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7000/20000, Loss: 807.188247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8020/20000 [01:25<01:52, 106.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8000/20000, Loss: 802.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9018/20000 [01:35<01:53, 97.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9000/20000, Loss: 797.832229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10018/20000 [01:46<01:41, 98.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000/20000, Loss: 810.631942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11019/20000 [01:56<01:32, 97.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11000/20000, Loss: 786.599018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12016/20000 [02:06<01:18, 101.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12000/20000, Loss: 812.610772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13015/20000 [02:16<01:08, 101.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13000/20000, Loss: 806.851819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14016/20000 [02:26<00:58, 102.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14000/20000, Loss: 817.807839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15012/20000 [02:36<00:52, 95.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15000/20000, Loss: 818.233565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16015/20000 [02:46<00:39, 100.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16000/20000, Loss: 795.617097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17019/20000 [02:55<00:28, 104.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17000/20000, Loss: 804.920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18016/20000 [03:04<00:18, 109.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18000/20000, Loss: 823.298742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19016/20000 [03:13<00:08, 109.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19000/20000, Loss: 804.943380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:22<00:00, 98.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000/20000, Loss: 806.105861\n",
      "\n",
      "评估漂移项网络...\n",
      "Drift relative L2 errors - a1: 8.7074e+00, a2: 1.2201e+01, total: 1.0711e+01\n",
      "漂移项网络相对L2误差: 1.0711e+01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.special import roots_legendre\n",
    "\n",
    "# 设置随机种子以确保可复现性\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def phi(x):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    return 1 + (2/15) * (4 * x1**2 - x1 * x2 + x2**2)\n",
    "\n",
    "# 定义漂移 a(x)\n",
    "def drift(x):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    a1 = -1.5 * x1 + x2\n",
    "    a2 = 0.25 * x1 - 1.5 * x2\n",
    "    return torch.stack([a1, a2], dim=1)\n",
    "\n",
    "\n",
    "# 定义扩散 b(x)\n",
    "def diffusion(x, d, device):\n",
    "    batch_size = x.shape[0]\n",
    "    sqrt_phi_val = torch.sqrt(phi(x))\n",
    "    \n",
    "    # 创建一个批处理的2x2矩阵\n",
    "    b_matrix = torch.zeros(batch_size, d, d, device=device)\n",
    "    \n",
    "    # 填充矩阵\n",
    "    b_matrix[:, 0, 0] = sqrt_phi_val\n",
    "    b_matrix[:, 0, 1] = 0\n",
    "    b_matrix[:, 1, 0] = -sqrt_phi_val * (11 / 8)\n",
    "    b_matrix[:, 1, 1] = (torch.sqrt(torch.tensor(255.0, device=device)) / 8) * sqrt_phi_val\n",
    "    \n",
    "    return b_matrix\n",
    "\n",
    "# 使用批处理和GPU加速的Euler-Maruyama方法\n",
    "def simulate_sde_batched(N, d, batch_size, delta_t, device):\n",
    "    # 结果数组，存储在CPU上\n",
    "    x_result = torch.zeros((N, d), dtype=torch.float64)\n",
    "    x_result[0] = torch.tensor([0.0, 0.0])  # 初始点\n",
    "    \n",
    "    # 当前状态，初始为[0,0]\n",
    "    x_current = torch.zeros((batch_size, d), device=device)\n",
    "    \n",
    "    # 使用批处理进行模拟\n",
    "    for n in tqdm(range(0, N-1, batch_size)):\n",
    "        # 确定当前批次的实际大小（最后一批可能小于batch_size）\n",
    "        current_batch_size = min(batch_size, N-1-n)\n",
    "        \n",
    "        if current_batch_size < batch_size:\n",
    "            x_current = x_current[:current_batch_size]\n",
    "        \n",
    "        # 漂移项\n",
    "        drift_term = drift(x_current) * delta_t\n",
    "        \n",
    "        # 扩散项\n",
    "        diffusion_matrices = diffusion(x_current, d, device)  # 形状: [batch_size, d, d]\n",
    "        noise = torch.randn(current_batch_size, d, device=device)  # 标准正态噪声\n",
    "        \n",
    "        # 对每个样本应用扩散矩阵\n",
    "        diffusion_term = torch.sqrt(torch.tensor(delta_t, device=device)) * torch.bmm(diffusion_matrices, noise.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # 更新状态\n",
    "        x_next = x_current + drift_term + diffusion_term\n",
    "        \n",
    "        # 将结果转移到CPU并存储\n",
    "        x_result[n+1:n+1+current_batch_size] = x_next.cpu()\n",
    "        \n",
    "        # 更新当前状态为新状态（用于下一个批次）\n",
    "        if n + batch_size < N - 1:\n",
    "            x_current = x_next.clone()\n",
    "        \n",
    "    return x_result\n",
    "\n",
    "\n",
    "# 2. 定义神经网络模型\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish激活函数: x * tanh(softplus(x))\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "class ReLU3(nn.Module):\n",
    "    \"\"\"ReLU3激活函数: min(max(0,x), 3)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(x), max=3.0)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"根据图片描述修改的 ResNet，修复维度不匹配问题\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=6, activation='mish'):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # 输入映射层：将 input_dim 映射到 hidden_dim\n",
    "        # self.input_map = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 每层的线性变换（只有一个线性层）\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Kaiming 初始化\n",
    "        # for layer in self.layers:\n",
    "        #     nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     nn.init.zeros_(layer.bias)\n",
    "        # nn.init.kaiming_normal_(self.output_layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # nn.init.zeros_(self.output_layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将输入映射到 hidden_dim\n",
    "        # 使用0将x从input_dim填充成hidden_dim\n",
    "        batch_size = x.shape[0]\n",
    "        x_mapped = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
    "        x_mapped[:, :x.shape[1]] = x  # 只填充前input_dim个维度，其余为0\n",
    "        \n",
    "        # 初始条件：h_0 = x_mapped, h_{-1} = 0\n",
    "        h = [torch.zeros_like(x_mapped), x_mapped]  # h[-1] = h_{-1}, h[-2] = h_0\n",
    "        \n",
    "        # 逐层计算\n",
    "        for ell in range(self.num_blocks):\n",
    "            # v_ell = sigma(W_ell * h_{ell-1} + g_ell)\n",
    "            v_ell = self.act(self.layers[ell](h[-1]))\n",
    "            # h_ell = pad(h_{ell-2}) + v_ell\n",
    "            h_ell = h[-2] + v_ell  # h1 = RLx; h2 = x + RLRLx; h3 = RL + RL(x + RLRLx); ...\n",
    "            h.append(h_ell)\n",
    "        \n",
    "        # 最终输出：c^T h_L\n",
    "        h_L = h[-1]\n",
    "        return self.output_layer(h_L)\n",
    "\n",
    "\n",
    "\n",
    "class DriftNet(nn.Module):\n",
    "    \"\"\"估计漂移项的网络 - 使用ReLU激活函数\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=50, output_dim=2, num_blocks=6):\n",
    "        super(DriftNet, self).__init__()\n",
    "        self.net = ResNet(input_dim, hidden_dim, output_dim, num_blocks, activation='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 3. 训练函数\n",
    "\n",
    "def train_drift_net_from_data(a_nn, x_data, y_data, num_iterations=20000, batch_size=10000, lr=1e-4, device=device):\n",
    "    \"\"\"训练漂移项网络 - 使用时间序列数据中的实际位移\"\"\"\n",
    "    print(\"训练漂移项网络...\")\n",
    "    a_nn.to(device)\n",
    "    optimizer = optim.Adam(a_nn.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iterations)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    # 将训练数据预先转移到GPU，减少每次迭代的数据传输开销\n",
    "    print(\"将训练数据预加载到GPU...\")\n",
    "    x_data_tensor = torch.tensor(x_data, dtype=torch.float64, device=device)\n",
    "    y_data_tensor = torch.tensor(y_data, dtype=torch.float64, device=device)\n",
    "    \n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        # 随机抽取批次\n",
    "        idx = torch.randint(0, len(x_data), (batch_size,), device=device)\n",
    "        x_batch = x_data_tensor[idx]\n",
    "        y_batch = y_data_tensor[idx]\n",
    "        \n",
    "        # 前向传播\n",
    "        a_pred = a_nn(x_batch)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = torch.mean((a_pred - y_batch) ** 2)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(a_nn.parameters(), max_norm=40.0)  # 限制梯度的L2范数不超过40.0\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6)):\n",
    "    \"\"\"生成二维高斯求积点\n",
    "    \n",
    "    参数:\n",
    "        nx, ny: 每个维度的高斯点数量\n",
    "        x_range, y_range: 积分区域的范围\n",
    "    \n",
    "    返回:\n",
    "        points: 形状为(nx*ny, 2)的高斯点坐标\n",
    "        weights: 对应的积分权重，形状为(nx*ny,)\n",
    "        X, Y: 重构的网格矩阵，用于可视化\n",
    "    \"\"\"\n",
    "    # 获取一维高斯-勒让德点和权重\n",
    "    x_points, x_weights = roots_legendre(nx)\n",
    "    y_points, y_weights = roots_legendre(ny)\n",
    "    \n",
    "    # 将[-1,1]范围映射到指定范围\n",
    "    x_min, x_max = x_range\n",
    "    y_min, y_max = y_range\n",
    "    \n",
    "    x_points = 0.5 * (x_max - x_min) * (x_points + 1) + x_min\n",
    "    y_points = 0.5 * (y_max - y_min) * (y_points + 1) + y_min\n",
    "    \n",
    "    # 计算二维点的笛卡尔积\n",
    "    xx, yy = np.meshgrid(x_points, y_points)\n",
    "    points = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "    \n",
    "    # 计算对应的权重（两个方向权重的张量积）\n",
    "    xx_weights, yy_weights = np.meshgrid(x_weights, y_weights)\n",
    "    weights = xx_weights.flatten() * yy_weights.flatten()\n",
    "    \n",
    "    # 调整权重，考虑积分区域的大小\n",
    "    area_factor = 0.25 * (x_max - x_min) * (y_max - y_min)\n",
    "    weights = weights * area_factor\n",
    "    \n",
    "    return points, weights, xx, yy\n",
    "\n",
    "def compute_relative_L2_error_with_quadrature(func_true, func_pred, grid_points, weights):\n",
    "    \"\"\"使用高斯求积计算相对L2误差\"\"\"\n",
    "    true_vals = func_true(grid_points)\n",
    "    pred_vals = func_pred(grid_points)\n",
    "    \n",
    "    # 使用权重计算加权均方误差\n",
    "    squared_diff = np.sum((true_vals - pred_vals)**2, axis=1)\n",
    "    error = np.sqrt(np.sum(squared_diff * weights))\n",
    "    \n",
    "    # 使用权重计算函数范数\n",
    "    squared_true = np.sum(true_vals**2, axis=1)\n",
    "    norm = np.sqrt(np.sum(squared_true * weights))\n",
    "    \n",
    "    return error / norm\n",
    "\n",
    "# 4. 评估函数\n",
    "def evaluate_drift_net(a_nn, grid_points, X, Y):\n",
    "    \"\"\"评估漂移项网络\n",
    "    \n",
    "    参数:\n",
    "        a_nn: 训练好的漂移网络模型\n",
    "        grid_points: 用于可视化的网格点\n",
    "        X, Y: 用于重构网格的坐标矩阵\n",
    "    \"\"\"\n",
    "    # 生成用于计算相对L2误差的10,000个高斯求积点\n",
    "    eval_points, eval_weights, _, _ = generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6))\n",
    "    \n",
    "    # 转换为张量\n",
    "    x_tensor = torch.tensor(grid_points, dtype=torch.float64, device=device)\n",
    "    eval_tensor = torch.tensor(eval_points, dtype=torch.float64, device=device)\n",
    "\n",
    "    # 计算真实值和预测值（用于可视化）\n",
    "    with torch.no_grad():\n",
    "        # 计算真实漂移值\n",
    "        a_true = drift(x_tensor).cpu().numpy()\n",
    "        a_pred = a_nn(x_tensor)\n",
    "        a_pred = a_pred.cpu().numpy() \n",
    "        \n",
    "        # 为L2误差计算获取评估点的真实值和预测值\n",
    "        a_true_eval = drift(eval_tensor).cpu().numpy()\n",
    "        a_pred_eval = a_nn(eval_tensor)\n",
    "        a_pred_eval = a_pred_eval.cpu().numpy()\n",
    "        \n",
    "    \n",
    "    # 计算相对L2误差\n",
    "    error_a1 = np.sqrt(np.mean((a_true_eval[:, 0] - a_pred_eval[:, 0])**2)) / np.sqrt(np.mean(a_true_eval[:, 0]**2))\n",
    "    error_a2 = np.sqrt(np.mean((a_true_eval[:, 1] - a_pred_eval[:, 1])**2)) / np.sqrt(np.mean(a_true_eval[:, 1]**2))\n",
    "    total_error = np.sqrt(np.mean(np.sum((a_true_eval - a_pred_eval)**2, axis=1))) / np.sqrt(np.mean(np.sum(a_true_eval**2, axis=1)))\n",
    "    \n",
    "    print(f\"Drift relative L2 errors - a1: {error_a1:.4e}, a2: {error_a2:.4e}, total: {total_error:.4e}\")\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 第一个分量\n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a1')\n",
    "    plt.title('True Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a1')\n",
    "    plt.title('Predicted Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 0] - a_pred[:, 0]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a1')\n",
    "    plt.title(f'Error in Drift a1 (L2 rel. error: {error_a1:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a1_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 第二个分量\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a2')\n",
    "    plt.title('True Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a2')\n",
    "    plt.title('Predicted Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 1] - a_pred[:, 1]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a2')\n",
    "    plt.title(f'Error in Drift a2 (L2 rel. error: {error_a2:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a2_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "# 5. 主函数\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"复现论文中的Student's t-distribution实验\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # 参数设置 - 按照论文要求\n",
    "    dt = 0.05  # 时间步长\n",
    "    d = 2   # 维度\n",
    "    N = 2*10**7  # 数据点数\n",
    "    hidden_dim = 50  # 隐藏层维度\n",
    "    num_blocks = 6   # 残差网络块数量（6层隐藏层）\n",
    "    data_batch_size = 10000  # 数据批处理大小\n",
    "    # 训练参数 - 根据论文设置\n",
    "    num_iterations = 20000  # 训练迭代次数\n",
    "    drift_batch_size = 10000      # 漂移项批处理大小\n",
    "    \n",
    "    # 学习率 - 根据论文设置\n",
    "    drift_lr = 1e-4        # 漂移项学习率\n",
    "    # 定义数据范围\n",
    "    omega_bounds = ([-4, 4], [-6, 6])  # x1, x2的边界\n",
    "\n",
    "    \n",
    "    # 1. 数据生成\n",
    "    print(\"生成数据...\")\n",
    "    x = simulate_sde_batched(N, d, data_batch_size, dt, device)\n",
    "    data = x.numpy()\n",
    "\n",
    "    # 数据筛选和预处理\n",
    "    print(\"筛选和预处理数据...\")\n",
    "    # 首先筛选出在指定范围内的点\n",
    "    mask_x1 = (data[:, 0] >= omega_bounds[0][0]) & (data[:, 0] <= omega_bounds[0][1]) \n",
    "    mask_x2 = (data[:, 1] >= omega_bounds[1][0]) & (data[:, 1] <= omega_bounds[1][1])\n",
    "    mask = mask_x1 & mask_x2\n",
    "    filtered_data = data[mask]\n",
    "    \n",
    "    # 准备输入和目标数据\n",
    "    N_filtered = len(filtered_data)\n",
    "    print(f\"筛选后的数据点数量: {N_filtered}\")\n",
    "    \n",
    "    # 使用CUDA加速计算位移\n",
    "    print(\"使用CUDA计算位移...\")\n",
    "    # 将数据转移到GPU\n",
    "    filtered_data_tensor = torch.tensor(filtered_data, dtype=torch.float64, device=device)\n",
    "    \n",
    "    # 使用张量操作计算位移 y = (x_{n+1} - x_n)/dt\n",
    "    x_data_tensor = filtered_data_tensor[:-1]  # 输入是前N-1个点\n",
    "    y_data_tensor = (filtered_data_tensor[1:] - filtered_data_tensor[:-1]) / dt\n",
    "    \n",
    "    # 转回CPU进行后续处理\n",
    "    x_data = x_data_tensor.cpu().numpy()\n",
    "    y_data = y_data_tensor.cpu().numpy()\n",
    "    \n",
    "    # 显示数据统计信息\n",
    "    print(f\"筛选后数据点数量: {x_data.shape[0]} ({x_data.shape[0]/len(data)*100:.2f}%)\")\n",
    "    # 检查 y_data 的数值范围\n",
    "    print(f\"筛选后数据统计: min={x_data.min(axis=0)}, max={x_data.max(axis=0)}, mean={x_data.mean(axis=0)}, std={x_data.std(axis=0)}\")\n",
    "    print(f\"位移数据统计: min={y_data.min(axis=0)}, max={y_data.max(axis=0)}, mean={y_data.mean(axis=0)}, std={y_data.std(axis=0)}\")\n",
    "    \n",
    "    # 生成用于可视化的网格点\n",
    "    print(\"生成网格点用于可视化和评估...\")\n",
    "    vis_grid_points, vis_weights, X, Y = generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6))\n",
    "\n",
    "    # 初始化漂移网络\n",
    "    drift_net = DriftNet(input_dim=2, hidden_dim=hidden_dim, output_dim=2, num_blocks=num_blocks).to(device)\n",
    "    \n",
    "    # 训练和评估模型\n",
    "    print(\"\\n\" + \"-\"*30)\n",
    "    print(\"第一步: 训练漂移项网络\")\n",
    "    print(\"-\"*30)\n",
    "    drift_losses = train_drift_net_from_data(\n",
    "        drift_net, \n",
    "        x_data,\n",
    "        y_data,\n",
    "        num_iterations=num_iterations, \n",
    "        batch_size=drift_batch_size, \n",
    "        lr=drift_lr\n",
    "    )\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(drift_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Drift Network Training Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results/drift_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 评估漂移项网络\n",
    "    print(\"\\n评估漂移项网络...\")\n",
    "    drift_error = evaluate_drift_net(drift_net, vis_grid_points, X, Y)\n",
    "    print(f\"漂移项网络相对L2误差: {drift_error:.4e}\")\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "复现论文中的Student's t-distribution实验\n",
      "==================================================\n",
      "\n",
      "生成数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1286.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选和预处理数据...\n",
      "筛选后的数据点数量: 19610021\n",
      "使用CUDA计算位移...\n",
      "创建归一化器...\n",
      "筛选后数据点数量: 19610020 (98.05%)\n",
      "筛选后数据统计: min=[-3.9999967 -5.999999 ], max=[3.9999666 5.999997 ], mean=[0.00065103 0.00119764], std=[0.9181222 1.7408456]\n",
      "位移数据统计: min=[-158.44101 -239.49367], max=[158.90303 238.28662], mean=[ 2.337964e-07 -7.263260e-07], std=[25.913853 49.165226]\n",
      "生成网格点用于可视化和评估...\n",
      "\n",
      "------------------------------\n",
      "第一步: 训练漂移项网络\n",
      "------------------------------\n",
      "训练漂移项网络...\n",
      "将训练数据预加载到GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 738\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# 执行主函数\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 583\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# 使用数据标准化和提前停止训练\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m drift_losses, drift_val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_drift_net_from_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrift_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrift_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrift_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalizer_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalizer_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_normalizer\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;66;03m# 评估漂移项网\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# 评估漂移项网络\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m评估漂移项网络...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 262\u001b[0m, in \u001b[0;36mtrain_drift_net_from_data\u001b[0;34m(a_nn, x_data, y_data, num_iterations, batch_size, lr, weight_decay, device, normalizer_x, normalizer_y)\u001b[0m\n\u001b[1;32m    259\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[idx]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m a_pred \u001b[38;5;241m=\u001b[39m \u001b[43ma_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m    265\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((a_pred \u001b[38;5;241m-\u001b[39m y_batch) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 207\u001b[0m, in \u001b[0;36mDriftNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 173\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# 将输入映射到 hidden_dim\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# 通过残差块\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/inverse-sde/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.special import roots_legendre\n",
    "\n",
    "# 设置随机种子以确保可复现性 - 取消注释以确保结果可重现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True  # 添加这行使CUDA操作也是确定性的\n",
    "torch.backends.cudnn.benchmark = False     # 关闭自动优化，确保结果可重现\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def phi(x):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    return 1 + (2/15) * (4 * x1**2 - x1 * x2 + x2**2)\n",
    "\n",
    "# 定义漂移 a(x)\n",
    "def drift(x):\n",
    "    x1, x2 = x[:, 0], x[:, 1]\n",
    "    a1 = -1.5 * x1 + x2\n",
    "    a2 = 0.25 * x1 - 1.5 * x2\n",
    "    return torch.stack([a1, a2], dim=1)\n",
    "\n",
    "# 定义扩散 b(x)\n",
    "def diffusion(x, d, device):\n",
    "    batch_size = x.shape[0]\n",
    "    sqrt_phi_val = torch.sqrt(phi(x))\n",
    "    \n",
    "    # 创建一个批处理的2x2矩阵\n",
    "    b_matrix = torch.zeros(batch_size, d, d, device=device)\n",
    "    \n",
    "    # 填充矩阵\n",
    "    b_matrix[:, 0, 0] = sqrt_phi_val\n",
    "    b_matrix[:, 0, 1] = 0\n",
    "    b_matrix[:, 1, 0] = -sqrt_phi_val * (11 / 8)\n",
    "    b_matrix[:, 1, 1] = (torch.sqrt(torch.tensor(255.0, device=device)) / 8) * sqrt_phi_val\n",
    "    \n",
    "    return b_matrix\n",
    "\n",
    "# 使用批处理和GPU加速的Euler-Maruyama方法\n",
    "def simulate_sde_batched(N, d, batch_size, delta_t, device):\n",
    "    # 结果数组，存储在CPU上\n",
    "    x_result = torch.zeros((N, d), dtype=torch.float64)\n",
    "    x_result[0] = torch.tensor([0.0, 0.0])  # 初始点\n",
    "    \n",
    "    # 当前状态，初始为[0,0]\n",
    "    x_current = torch.zeros((batch_size, d), device=device)\n",
    "    \n",
    "    # 使用批处理进行模拟\n",
    "    for n in tqdm(range(0, N-1, batch_size)):\n",
    "        # 确定当前批次的实际大小（最后一批可能小于batch_size）\n",
    "        current_batch_size = min(batch_size, N-1-n)\n",
    "        \n",
    "        if current_batch_size < batch_size:\n",
    "            x_current = x_current[:current_batch_size]\n",
    "        \n",
    "        # 漂移项\n",
    "        drift_term = drift(x_current) * delta_t\n",
    "        \n",
    "        # 扩散项\n",
    "        diffusion_matrices = diffusion(x_current, d, device)  # 形状: [batch_size, d, d]\n",
    "        noise = torch.randn(current_batch_size, d, device=device)  # 标准正态噪声\n",
    "        \n",
    "        # 对每个样本应用扩散矩阵\n",
    "        diffusion_term = torch.sqrt(torch.tensor(delta_t, device=device)) * torch.bmm(diffusion_matrices, noise.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # 更新状态\n",
    "        x_next = x_current + drift_term + diffusion_term\n",
    "        \n",
    "        # 将结果转移到CPU并存储\n",
    "        x_result[n+1:n+1+current_batch_size] = x_next.cpu()\n",
    "        \n",
    "        # 更新当前状态为新状态（用于下一个批次）\n",
    "        if n + batch_size < N - 1:\n",
    "            x_current = x_next.clone()\n",
    "        \n",
    "    return x_result\n",
    "\n",
    "# 添加数据标准化类\n",
    "class Normalizer:\n",
    "    def __init__(self, data):\n",
    "        self.mean = torch.mean(data, dim=0)\n",
    "        self.std = torch.std(data, dim=0)\n",
    "        self.std[self.std < 1e-8] = 1.0  # 防止除以零\n",
    "        \n",
    "    def normalize(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "    \n",
    "    def denormalize(self, data):\n",
    "        return data * self.std + self.mean\n",
    "\n",
    "# 2. 定义神经网络模型\n",
    "class Mish(nn.Module):\n",
    "    \"\"\"Mish激活函数: x * tanh(softplus(x))\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "class ReLU3(nn.Module):\n",
    "    \"\"\"ReLU3激活函数: min(max(0,x), 3)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(x), max=3.0)\n",
    "\n",
    "# 改进的ResNet模型\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"改进的ResNet，优化了结构并添加了批归一化和dropout\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=6, activation='relu', dropout_rate=0.1):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # 输入映射层：将 input_dim 映射到 hidden_dim\n",
    "        self.input_map = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 残差块\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            block = nn.ModuleDict({\n",
    "                'norm1': nn.BatchNorm1d(hidden_dim),\n",
    "                'linear1': nn.Linear(hidden_dim, hidden_dim),\n",
    "                'norm2': nn.BatchNorm1d(hidden_dim),\n",
    "                'linear2': nn.Linear(hidden_dim, hidden_dim),\n",
    "                'dropout': nn.Dropout(dropout_rate)\n",
    "            })\n",
    "            self.blocks.append(block)\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'mish':\n",
    "            self.act = Mish()\n",
    "        elif activation == 'relu3':\n",
    "            self.act = ReLU3()\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的激活函数: {activation}\")\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # 更高级的初始化\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        # 为输入映射层初始化\n",
    "        nn.init.xavier_uniform_(self.input_map.weight)\n",
    "        nn.init.zeros_(self.input_map.bias)\n",
    "        \n",
    "        # 为每个残差块初始化\n",
    "        for block in self.blocks:\n",
    "            nn.init.xavier_uniform_(block['linear1'].weight)\n",
    "            nn.init.zeros_(block['linear1'].bias)\n",
    "            nn.init.xavier_uniform_(block['linear2'].weight)\n",
    "            nn.init.zeros_(block['linear2'].bias)\n",
    "        \n",
    "        # 为输出层初始化\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将输入映射到 hidden_dim\n",
    "        h = self.input_map(x)\n",
    "        \n",
    "        # 通过残差块\n",
    "        for block in self.blocks:\n",
    "            residual = h\n",
    "            \n",
    "            # 第一个线性层+激活\n",
    "            h = block['norm1'](h)\n",
    "            h = self.act(block['linear1'](h))\n",
    "            \n",
    "            # 第二个线性层\n",
    "            h = block['norm2'](h)\n",
    "            h = block['linear2'](h)\n",
    "            \n",
    "            # 添加残差连接\n",
    "            h = h + residual\n",
    "            \n",
    "            # Dropout\n",
    "            h = block['dropout'](h)\n",
    "            \n",
    "            # 激活\n",
    "            h = self.act(h)\n",
    "        \n",
    "        # 最终输出\n",
    "        return self.output_layer(h)\n",
    "\n",
    "class DriftNet(nn.Module):\n",
    "    \"\"\"估计漂移项的网络 - 使用改进的ResNet\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=100, output_dim=2, num_blocks=6, dropout_rate=0.1):\n",
    "        super(DriftNet, self).__init__()\n",
    "        self.net = ResNet(input_dim, hidden_dim, output_dim, num_blocks, \n",
    "                          activation='relu', dropout_rate=dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 3. 训练函数\n",
    "def train_drift_net_from_data(a_nn, x_data, y_data, num_iterations=20000, batch_size=10000, lr=1e-3, \n",
    "                             weight_decay=1e-5, device=device, normalizer_x=None, normalizer_y=None):\n",
    "    \"\"\"训练漂移项网络 - 使用时间序列数据中的实际位移\"\"\"\n",
    "    print(\"训练漂移项网络...\")\n",
    "    a_nn.to(device)\n",
    "    \n",
    "    # 使用Adam优化器并添加权重衰减 (L2正则化)\n",
    "    optimizer = optim.Adam(a_nn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # 使用余弦退火学习率调度器\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iterations)\n",
    "    \n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # 分割数据为训练集和验证集 (90%-10%)\n",
    "    n_samples = len(x_data)\n",
    "    n_train = int(0.9 * n_samples)\n",
    "    \n",
    "    # 随机置换索引\n",
    "    indices = torch.randperm(n_samples)\n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:]\n",
    "    \n",
    "    # 将训练数据预先转移到GPU\n",
    "    print(\"将训练数据预加载到GPU...\")\n",
    "    x_data_tensor = torch.tensor(x_data, dtype=torch.float32, device=device)\n",
    "    y_data_tensor = torch.tensor(y_data, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 如果提供了归一化器，使用它们\n",
    "    if normalizer_x is not None and normalizer_y is not None:\n",
    "        x_data_tensor = normalizer_x.normalize(x_data_tensor)\n",
    "        y_data_tensor = normalizer_y.normalize(y_data_tensor)\n",
    "    \n",
    "    x_train = x_data_tensor[train_indices]\n",
    "    y_train = y_data_tensor[train_indices]\n",
    "    x_val = x_data_tensor[val_indices]\n",
    "    y_val = y_data_tensor[val_indices]\n",
    "    \n",
    "    # 提前停止设置\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 500  # 提前停止的耐心值\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        # 随机抽取批次进行训练\n",
    "        idx = torch.randint(0, len(x_train), (batch_size,), device=device)\n",
    "        x_batch = x_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        \n",
    "        # 前向传播\n",
    "        a_pred = a_nn(x_batch)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = torch.mean((a_pred - y_batch) ** 2)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 启用梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(a_nn.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # 每100次迭代评估一次验证集\n",
    "        if (i+1) % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                # 由于验证集可能很大，分批处理\n",
    "                val_loss = 0\n",
    "                num_batches = 0\n",
    "                for j in range(0, len(x_val), batch_size):\n",
    "                    x_val_batch = x_val[j:j+batch_size]\n",
    "                    y_val_batch = y_val[j:j+batch_size]\n",
    "                    \n",
    "                    val_pred = a_nn(x_val_batch)\n",
    "                    batch_loss = torch.mean((val_pred - y_val_batch) ** 2).item()\n",
    "                    val_loss += batch_loss\n",
    "                    num_batches += 1\n",
    "                \n",
    "                val_loss /= num_batches\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                # 检查是否需要保存最佳模型\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = a_nn.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                # 提前停止检查\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"提前停止在迭代 {i+1}/{num_iterations}，最佳验证损失: {best_val_loss:.6f}\")\n",
    "                    a_nn.load_state_dict(best_model_state)\n",
    "                    break\n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"迭代 {i+1}/{num_iterations}, 训练损失: {loss.item():.6f}, 学习率: {scheduler.get_last_lr()[0]:.6e}\")\n",
    "    \n",
    "    # 如果没有提前停止，加载最佳模型\n",
    "    if best_model_state is not None and patience_counter < patience:\n",
    "        a_nn.load_state_dict(best_model_state)\n",
    "    \n",
    "    # 绘制训练和验证损失\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Drift Network Training Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(0, len(val_losses)*100, 100), val_losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Drift Network Validation Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_training_validation_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return losses, val_losses\n",
    "\n",
    "def generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6)):\n",
    "    \"\"\"生成二维高斯求积点\n",
    "    \n",
    "    参数:\n",
    "        nx, ny: 每个维度的高斯点数量\n",
    "        x_range, y_range: 积分区域的范围\n",
    "    \n",
    "    返回:\n",
    "        points: 形状为(nx*ny, 2)的高斯点坐标\n",
    "        weights: 对应的积分权重，形状为(nx*ny,)\n",
    "        X, Y: 重构的网格矩阵，用于可视化\n",
    "    \"\"\"\n",
    "    # 获取一维高斯-勒让德点和权重\n",
    "    x_points, x_weights = roots_legendre(nx)\n",
    "    y_points, y_weights = roots_legendre(ny)\n",
    "    \n",
    "    # 将[-1,1]范围映射到指定范围\n",
    "    x_min, x_max = x_range\n",
    "    y_min, y_max = y_range\n",
    "    \n",
    "    x_points = 0.5 * (x_max - x_min) * (x_points + 1) + x_min\n",
    "    y_points = 0.5 * (y_max - y_min) * (y_points + 1) + y_min\n",
    "    \n",
    "    # 计算二维点的笛卡尔积\n",
    "    xx, yy = np.meshgrid(x_points, y_points)\n",
    "    points = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "    \n",
    "    # 计算对应的权重（两个方向权重的张量积）\n",
    "    xx_weights, yy_weights = np.meshgrid(x_weights, y_weights)\n",
    "    weights = xx_weights.flatten() * yy_weights.flatten()\n",
    "    \n",
    "    # 调整权重，考虑积分区域的大小\n",
    "    area_factor = 0.25 * (x_max - x_min) * (y_max - y_min)\n",
    "    weights = weights * area_factor\n",
    "    \n",
    "    return points, weights, xx, yy\n",
    "\n",
    "def compute_relative_L2_error_with_quadrature(func_true, func_pred, grid_points, weights):\n",
    "    \"\"\"使用高斯求积计算相对L2误差\"\"\"\n",
    "    true_vals = func_true(grid_points)\n",
    "    pred_vals = func_pred(grid_points)\n",
    "    \n",
    "    # 使用权重计算加权均方误差\n",
    "    squared_diff = np.sum((true_vals - pred_vals)**2, axis=1)\n",
    "    error = np.sqrt(np.sum(squared_diff * weights))\n",
    "    \n",
    "    # 使用权重计算函数范数\n",
    "    squared_true = np.sum(true_vals**2, axis=1)\n",
    "    norm = np.sqrt(np.sum(squared_true * weights))\n",
    "    \n",
    "    return error / norm\n",
    "\n",
    "# 4. 评估函数\n",
    "def evaluate_drift_net(a_nn, grid_points, X, Y, normalizer_x=None):\n",
    "    \"\"\"评估漂移项网络\n",
    "    \n",
    "    参数:\n",
    "        a_nn: 训练好的漂移网络模型\n",
    "        grid_points: 用于可视化的网格点\n",
    "        X, Y: 用于重构网格的坐标矩阵\n",
    "        normalizer_x: 输入数据的归一化器\n",
    "    \"\"\"\n",
    "    # 生成用于计算相对L2误差的10,000个高斯求积点\n",
    "    eval_points, eval_weights, _, _ = generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6))\n",
    "    \n",
    "    # 转换为张量\n",
    "    x_tensor = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "    eval_tensor = torch.tensor(eval_points, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 如果提供了归一化器，应用它\n",
    "    if normalizer_x is not None:\n",
    "        x_tensor_norm = normalizer_x.normalize(x_tensor)\n",
    "        eval_tensor_norm = normalizer_x.normalize(eval_tensor)\n",
    "    else:\n",
    "        x_tensor_norm = x_tensor\n",
    "        eval_tensor_norm = eval_tensor\n",
    "\n",
    "    # 计算真实值和预测值（用于可视化）\n",
    "    with torch.no_grad():\n",
    "        # 计算真实漂移值\n",
    "        a_true = drift(x_tensor).cpu().numpy()\n",
    "        a_pred = a_nn(x_tensor_norm)\n",
    "        a_pred = a_pred.cpu().numpy() \n",
    "        \n",
    "        # 为L2误差计算获取评估点的真实值和预测值\n",
    "        a_true_eval = drift(eval_tensor).cpu().numpy()\n",
    "        a_pred_eval = a_nn(eval_tensor_norm)\n",
    "        a_pred_eval = a_pred_eval.cpu().numpy()\n",
    "    \n",
    "    # 计算相对L2误差\n",
    "    error_a1 = np.sqrt(np.mean((a_true_eval[:, 0] - a_pred_eval[:, 0])**2)) / np.sqrt(np.mean(a_true_eval[:, 0]**2))\n",
    "    error_a2 = np.sqrt(np.mean((a_true_eval[:, 1] - a_pred_eval[:, 1])**2)) / np.sqrt(np.mean(a_true_eval[:, 1]**2))\n",
    "    total_error = np.sqrt(np.mean(np.sum((a_true_eval - a_pred_eval)**2, axis=1))) / np.sqrt(np.mean(np.sum(a_true_eval**2, axis=1)))\n",
    "    \n",
    "    print(f\"Drift relative L2 errors - a1: {error_a1:.4e}, a2: {error_a2:.4e}, total: {total_error:.4e}\")\n",
    "    \n",
    "    # 绘制结果\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 第一个分量\n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a1')\n",
    "    plt.title('True Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 0].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a1')\n",
    "    plt.title('Predicted Drift a1')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 0] - a_pred[:, 0]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a1')\n",
    "    plt.title(f'Error in Drift a1 (L2 rel. error: {error_a1:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a1_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 第二个分量\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.contourf(X, Y, a_true[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='True a2')\n",
    "    plt.title('True Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.contourf(X, Y, a_pred[:, 1].reshape(X.shape), 50, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted a2')\n",
    "    plt.title('Predicted Drift a2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.contourf(X, Y, (a_true[:, 1] - a_pred[:, 1]).reshape(X.shape), 50, cmap='coolwarm')\n",
    "    plt.colorbar(label='Error a2')\n",
    "    plt.title(f'Error in Drift a2 (L2 rel. error: {error_a2:.4e})')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_a2_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return total_error\n",
    "\n",
    "# 5. 主函数\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"复现论文中的Student's t-distribution实验\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # 参数设置 - 按照论文要求\n",
    "    dt = 0.05  # 时间步长\n",
    "    d = 2   # 维度\n",
    "    N = 2*10**7  # 数据点数\n",
    "    hidden_dim = 50  # 隐藏层维度（增加到100）\n",
    "    num_blocks = 6   # 残差网络块数量（6层隐藏层）\n",
    "    data_batch_size = 10000  # 数据批处理大小\n",
    "    \n",
    "    # 训练参数 - 改进设置\n",
    "    num_iterations = 20000  # 增加训练迭代次数\n",
    "    drift_batch_size = 1000  # 降低批次大小，提高泛化性\n",
    "    \n",
    "    # 学习率 - 增加初始学习率\n",
    "    drift_lr = 1e-4  # 漂移项学习率\n",
    "    \n",
    "    # L2正则化权重衰减\n",
    "    weight_decay = 1e-5\n",
    "    \n",
    "    # dropout率\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # 定义数据范围\n",
    "    omega_bounds = ([-4, 4], [-6, 6])  # x1, x2的边界\n",
    "    \n",
    "    # 1. 数据生成\n",
    "    print(\"生成数据...\")\n",
    "    x = simulate_sde_batched(N, d, data_batch_size, dt, device)\n",
    "    data = x.numpy()\n",
    "\n",
    "    # 数据筛选和预处理\n",
    "    print(\"筛选和预处理数据...\")\n",
    "    # 首先筛选出在指定范围内的点\n",
    "    mask_x1 = (data[:, 0] >= omega_bounds[0][0]) & (data[:, 0] <= omega_bounds[0][1]) \n",
    "    mask_x2 = (data[:, 1] >= omega_bounds[1][0]) & (data[:, 1] <= omega_bounds[1][1])\n",
    "    mask = mask_x1 & mask_x2\n",
    "    filtered_data = data[mask]\n",
    "    \n",
    "    # 准备输入和目标数据\n",
    "    N_filtered = len(filtered_data)\n",
    "    print(f\"筛选后的数据点数量: {N_filtered}\")\n",
    "    \n",
    "    # 使用CUDA加速计算位移\n",
    "    print(\"使用CUDA计算位移...\")\n",
    "    # 将数据转移到GPU\n",
    "    filtered_data_tensor = torch.tensor(filtered_data, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 使用张量操作计算位移 y = (x_{n+1} - x_n)/dt\n",
    "    x_data_tensor = filtered_data_tensor[:-1]  # 输入是前N-1个点\n",
    "    y_data_tensor = (filtered_data_tensor[1:] - filtered_data_tensor[:-1]) / dt\n",
    "    \n",
    "    # 创建归一化器\n",
    "    print(\"创建归一化器...\")\n",
    "    x_normalizer = Normalizer(x_data_tensor)\n",
    "    y_normalizer = Normalizer(y_data_tensor)\n",
    "    \n",
    "    # 转回CPU进行后续处理\n",
    "    x_data = x_data_tensor.cpu().numpy()\n",
    "    y_data = y_data_tensor.cpu().numpy()\n",
    "    \n",
    "    # 显示数据统计信息\n",
    "    print(f\"筛选后数据点数量: {x_data.shape[0]} ({x_data.shape[0]/len(data)*100:.2f}%)\")\n",
    "    # 检查 y_data 的数值范围\n",
    "    print(f\"筛选后数据统计: min={x_data.min(axis=0)}, max={x_data.max(axis=0)}, mean={x_data.mean(axis=0)}, std={x_data.std(axis=0)}\")\n",
    "    print(f\"位移数据统计: min={y_data.min(axis=0)}, max={y_data.max(axis=0)}, mean={y_data.mean(axis=0)}, std={y_data.std(axis=0)}\")\n",
    "    \n",
    "    # 生成用于可视化的网格点\n",
    "    print(\"生成网格点用于可视化和评估...\")\n",
    "    vis_grid_points, vis_weights, X, Y = generate_gauss_quadrature_points(nx=100, ny=100, x_range=(-4, 4), y_range=(-6, 6))\n",
    "\n",
    "    # 初始化漂移网络（增加隐藏层大小和添加dropout）\n",
    "    drift_net = DriftNet(input_dim=2, hidden_dim=hidden_dim, output_dim=2, \n",
    "                         num_blocks=num_blocks, dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # 训练和评估模型\n",
    "    print(\"\\n\" + \"-\"*30)\n",
    "    print(\"第一步: 训练漂移项网络\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # 使用数据标准化和提前停止训练\n",
    "    drift_losses, drift_val_losses = train_drift_net_from_data(\n",
    "        drift_net, \n",
    "        x_data,\n",
    "        y_data,\n",
    "        num_iterations=num_iterations, \n",
    "        batch_size=drift_batch_size, \n",
    "        lr=drift_lr,\n",
    "        weight_decay=weight_decay,\n",
    "        normalizer_x=x_normalizer,\n",
    "        normalizer_y=y_normalizer\n",
    "    )\n",
    "    \n",
    "    # 评估漂移项网\n",
    "    # 评估漂移项网络\n",
    "    print(\"\\n评估漂移项网络...\")\n",
    "    drift_error = evaluate_drift_net(drift_net, vis_grid_points, X, Y, normalizer_x=x_normalizer)\n",
    "    print(f\"漂移项网络相对L2误差: {drift_error:.4e}\")\n",
    "    \n",
    "    # 保存训练好的模型\n",
    "    print(\"保存模型...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': drift_net.state_dict(),\n",
    "        'normalizer_x_mean': x_normalizer.mean.cpu(),\n",
    "        'normalizer_x_std': x_normalizer.std.cpu(),\n",
    "        'normalizer_y_mean': y_normalizer.mean.cpu(),\n",
    "        'normalizer_y_std': y_normalizer.std.cpu(),\n",
    "    }, 'results/drift_net_model.pth')\n",
    "    \n",
    "    # 可视化训练过程中的预测情况\n",
    "    print(\"可视化训练效果...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 选择几个随机点进行可视化\n",
    "    vis_indices = np.random.choice(len(x_data), min(1000, len(x_data)), replace=False)\n",
    "    x_vis = torch.tensor(x_data[vis_indices], dtype=torch.float32, device=device)\n",
    "    y_vis = torch.tensor(y_data[vis_indices], dtype=torch.float32, device=device)\n",
    "    \n",
    "    # 使用网络进行预测\n",
    "    x_vis_norm = x_normalizer.normalize(x_vis)\n",
    "    with torch.no_grad():\n",
    "        y_pred = drift_net(x_vis_norm).cpu().numpy()\n",
    "    \n",
    "    y_vis = y_vis.cpu().numpy()\n",
    "    \n",
    "    # 可视化第一个分量\n",
    "    plt.subplot(221)\n",
    "    plt.scatter(y_vis[:, 0], y_pred[:, 0], alpha=0.3)\n",
    "    min_val = min(y_vis[:, 0].min(), y_pred[:, 0].min())\n",
    "    max_val = max(y_vis[:, 0].max(), y_pred[:, 0].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.xlabel('True a1')\n",
    "    plt.ylabel('Predicted a1')\n",
    "    plt.title('Drift Component a1')\n",
    "    plt.axis('square')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 可视化第二个分量\n",
    "    plt.subplot(222)\n",
    "    plt.scatter(y_vis[:, 1], y_pred[:, 1], alpha=0.3)\n",
    "    min_val = min(y_vis[:, 1].min(), y_pred[:, 1].min())\n",
    "    max_val = max(y_vis[:, 1].max(), y_pred[:, 1].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    plt.xlabel('True a2')\n",
    "    plt.ylabel('Predicted a2')\n",
    "    plt.title('Drift Component a2')\n",
    "    plt.axis('square')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 计算并显示每个分量的相关性\n",
    "    corr_a1 = np.corrcoef(y_vis[:, 0], y_pred[:, 0])[0, 1]\n",
    "    corr_a2 = np.corrcoef(y_vis[:, 1], y_pred[:, 1])[0, 1]\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.text(0.5, 0.5, f\"Correlation for a1: {corr_a1:.4f}\\nCorrelation for a2: {corr_a2:.4f}\", \n",
    "             ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 计算并显示整体误差\n",
    "    mse_a1 = np.mean((y_vis[:, 0] - y_pred[:, 0])**2)\n",
    "    mse_a2 = np.mean((y_vis[:, 1] - y_pred[:, 1])**2)\n",
    "    relative_error_a1 = np.sqrt(mse_a1) / np.std(y_vis[:, 0])\n",
    "    relative_error_a2 = np.sqrt(mse_a2) / np.std(y_vis[:, 1])\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.text(0.5, 0.5, f\"MSE for a1: {mse_a1:.4e}\\nMSE for a2: {mse_a2:.4e}\\n\" +\n",
    "             f\"Relative error a1: {relative_error_a1:.4e}\\nRelative error a2: {relative_error_a2:.4e}\", \n",
    "             ha='center', va='center', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/drift_prediction_scatter.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 绘制样本轨迹与预测比较\n",
    "    print(\"绘制样本轨迹比较...\")\n",
    "    \n",
    "    # 从原始数据中抽取一段轨迹\n",
    "    traj_start = np.random.randint(0, len(filtered_data) - 1000)\n",
    "    traj_length = 1000\n",
    "    traj_data = filtered_data[traj_start:traj_start+traj_length]\n",
    "    \n",
    "    # 计算原始和预测的变化率\n",
    "    traj_x = torch.tensor(traj_data[:-1], dtype=torch.float32, device=device)\n",
    "    traj_y_true = (traj_data[1:] - traj_data[:-1]) / dt\n",
    "    \n",
    "    traj_x_norm = x_normalizer.normalize(traj_x)\n",
    "    with torch.no_grad():\n",
    "        traj_y_pred = drift_net(traj_x_norm).cpu().numpy()\n",
    "    \n",
    "    # 绘制轨迹比较\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    time_steps = np.arange(len(traj_data)-1) * dt\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(time_steps, traj_data[:-1, 0], label='Position x1')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Trajectory Position x1')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(time_steps, traj_data[:-1, 1], label='Position x2')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Trajectory Position x2')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.plot(time_steps, traj_y_true[:, 0], label='True velocity')\n",
    "    plt.plot(time_steps, traj_y_pred[:, 0], label='Predicted velocity', linestyle='--')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Trajectory Velocity x1')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.plot(time_steps, traj_y_true[:, 1], label='True velocity')\n",
    "    plt.plot(time_steps, traj_y_pred[:, 1], label='Predicted velocity', linestyle='--')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Velocity')\n",
    "    plt.title('Trajectory Velocity x2')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/trajectory_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n实验完成！结果已保存到 results/ 目录\")\n",
    "\n",
    "# 执行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse-sde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
